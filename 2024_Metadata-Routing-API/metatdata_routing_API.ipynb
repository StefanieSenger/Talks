{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Scikit-learn's Metadata Routing API\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "This talk will introduce scikit-learn users to the new API for <b>metadata routing</b>, a feature introduced in the recent releases and almost fully available since version 1.5 (released in May 2024).\n",
    "\n",
    "We will explore what metadata is, how it can be used in machine learning pipelines, and how the new API simplifies routing metadata throughout a workflow. Routing metadata refers to an internal mechanism to pass metadata around between components of a data science pipeline, ensuring it reaches the functions that consume or utilize it.\n",
    "\n",
    "Using well-known metadata such as `sample_weight` and `groups` which are implemented in many scikit-learn metrics and evaluation tools, we will examine the restrictions for passing metadata prior to the introduction of the new API. Then, we will enable the new routing API and demonstrate how it solves these challenges with examples that involve layers of nested-ness through cross-validation, hyperparameter tuning, or pipelines. We will explain the core components of the API, including methods like `set_fit_request()` and how to actually pass our metadata.\n",
    "\n",
    "Attendees will leave with an understanding of how to enable and use the new routing API including passing metadata through Pipeline objects and validation tools like `cross_validate`. Additional references to the metadata user guide and developer guide will be provided for those interested in further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Metadata Routing?\n",
    "\n",
    "- definition of metadata:\n",
    "    - metadata can be any data, that we want to apply on top of our tabular data, without it necessarily being part of it\n",
    "    - alternative: metadata is any data, that some function in a data science \"pipeline/process\" handles besides from X and y; it's a param that influences this function's treatment of the data\n",
    "\n",
    "- examples for metadata:\n",
    "    - you might know from scikit-learn: `sample_weight` and `groups`\n",
    "    - but other libraries offer support for other kinds of metadata\n",
    "    - (slide only: graphically show some other examples of metadata: gender, race, sex, zipcode, .... and area/library it is used in)\n",
    "    - self defined metadata to be used in custom metrics (and possibly custom estimators)\n",
    "\n",
    "- use cases for metadata:\n",
    "    - fairness related use case\n",
    "    - business logic\n",
    "    - `sample_weight` and can be used to re-balance data and `groups` prevent data leakage in a cross validation\n",
    "    - note: we don't have better results if we pass metadata in terms of a better score, but we will have a more realistic model\n",
    "\n",
    "- definition of routing:\n",
    "    - routing just means that we pass metadata around between several components involved in a data science pipeline to where the metadata is used or consumed\n",
    "\n",
    "- prior to metadata routing API:\n",
    "    - we were restricted to only use `sample_weight` only within the metrics that would define it\n",
    "    - we could not consistently use it in a more nested structure\n",
    "\n",
    "- with metadata routing API:\n",
    "    - we can pass `sample_weight` and `groups` through several levels of nested-ness between the components of a data science pipeline\n",
    "    - we can combine objects from other libraries with scikit-learn estimators while still passing their metadata\n",
    "    - we can define our own custom metrics using self defined metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Passing metadata without the routing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>class</th>\n",
       "      <th>severity</th>\n",
       "      <th>medication</th>\n",
       "      <th>recovery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sex  age  race  class  severity  medication  recovery_time\n",
       "0     0   88     1      1         6           1             33\n",
       "1     1   71     1      1         1           1              8\n",
       "2     0  100     1      1         4           0             32\n",
       "3     0   87     1      0         0           1             56\n",
       "4     0   50     0      0         7           1             28\n",
       "..  ...  ...   ...    ...       ...         ...            ...\n",
       "95    1   71     1      1         9           1             16\n",
       "96    1   99     1      0         0           1             26\n",
       "97    0   38     1      1         1           0              0\n",
       "98    1   75     0      0         2           0              0\n",
       "99    1   58     0      1        10           1             24\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Note: To keep this example simple, we will simplify categorical data and only show two\n",
    "# sexes, two races, and two social-economic classes, which spares us from\n",
    "# one-hot-encoding categorical data. This example DataFrame doesn't mean to reflect\n",
    "# reality and is supposed to be readable on a slide during a presentation.\n",
    "\n",
    "rng = np.random.RandomState(41)\n",
    "sex = rng.randint(0,2,size=100)\n",
    "age = rng.normal(loc=70, scale=20, size=100)\n",
    "age = np.clip(age, 0, 100).astype(int) \n",
    "race = rng.choice([0, 1], size=100, p=[0.7, 0.3])\n",
    "social_class = rng.choice([0, 1], size=100, p=[0.6, 0.4])\n",
    "severity = rng.choice(range(11), size=100)\n",
    "medication = rng.randint(0,2,size=100)\n",
    "recovery_time = rng.normal(loc=25, scale=20, size=100)\n",
    "recovery_time = np.clip(recovery_time, 0, 200).astype(int)\n",
    "\n",
    "data = pd.DataFrame({\"sex\": sex, \"age\": age, \"race\": race, \"class\": social_class, \"severity\": severity, \"medication\": medication, \"recovery_time\": recovery_time})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].to_numpy()\n",
    "y = data.iloc[:,-1].to_numpy()\n",
    "\n",
    "groups = rng.randint(0, 10, size=X.shape[0])\n",
    "sample_weight = rng.rand(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example: medical study on the effectiveness of a treatment\n",
    "    - we would want to group the hospitals using the `groups` parameter scikit-learn offers\n",
    "        - we assume that each hospital’s data may have systematic biases due to factors like medical devices, policies, socioeconomic status of the patients, ...\n",
    "        - since a hospital is a collection of patients, we group the samples per hospital\n",
    "    - same hospital should not be both in the train and in the validation set when we cross validate\n",
    "    - `groups` is a metadata that is used in splitters exclusively, to make sure that if patterns exist within the data, we don’t leak those patterns between train and validation set, because we want to train our models on the targets and not on other patterns within the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00191188, 0.00093627]),\n",
       " 'score_time': array([0.00070477, 0.00059056]),\n",
       " 'test_score': array([-0.2923336 , -0.12131859]),\n",
       " 'train_score': array([0.21447416, 0.06814853])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_validate\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "# GroupKFold considers groups while splitting:\n",
    "cv = GroupKFold(n_splits=2)\n",
    "\n",
    "# providing `groups` for GroupKFold splitter:\n",
    "cross_validate(ridge, X, y, cv=cv, groups=groups, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we might also want to pass `sample_weight`\n",
    "\n",
    "- we would use `sample_weight` when we want to draw the attention of the model to a specific sub-group of samples, that our data miss-represents in some way, or to a problem with their distribution in respect to another feature\n",
    "\n",
    "- two ways of getting data\n",
    "    - randomised trail under very constrained conditions\n",
    "    - from real worlds observations, and then we can bring the statistic on top of data we train on\n",
    "\n",
    "- in our example talking about a medical study on the effectiveness of a treatment, `sample_weight` might\n",
    "    - encode sex or race of a patient (to balance out data) \n",
    "    - or if we suspect a correlation between a feature (like race) and the fact if a patient got the specific treatment, e.g. a bias in which patient was chosen for the treatment, then `sample_weight` could be used to counter-balance that\n",
    "        - there are several methods to determine `sample_weight` (from calculating proportions or more enhanced statistics from the data, or using more general statistical principles or natural laws that we know will take effect)\n",
    "\n",
    "-  giving certain samples a higher `sample_weight` than the rest will effect in minimizing the error or loss of the predictions for these samples more than the general error, while we are still taking the richness of all the data into account\n",
    "\n",
    "- we could pass `sample_weight` into the fit method and into the scoring:\n",
    "    - and it would be both taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-234.7176229868505)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "# Ridge().fit() can consume `sample_weight`:\n",
    "ridge = Ridge().fit(X,y, sample_weight=sample_weight) \n",
    "\n",
    "# `mean_squared_error` can consume sample_weight:\n",
    "scoring = get_scorer(\"neg_mean_squared_error\")\n",
    "\n",
    "# thus we pass `sample_weight` to the fitted ridge object and to the scorer:\n",
    "scoring(ridge, X, y, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but trying to pass `sample_weight` into `Ridge().fit()` within a cross validation results in it being ignored:\n",
    "    - the results are the same as not passing it\n",
    "    - because it is not passed to the different splits of the data when Ridge is refitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00111079, 0.00096726]),\n",
       " 'score_time': array([0.00051475, 0.0003984 ]),\n",
       " 'test_score': array([-365.16178094, -279.02891722]),\n",
       " 'train_score': array([-195.47024953, -263.3039523 ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, cross_validate\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "# Ridge().fit() can consume `sample_weight`, but when passed into `cross_validate` `sample_weight` is ignored:\n",
    "ridge = Ridge().fit(X,y, sample_weight=sample_weight)\n",
    "\n",
    "scoring = get_scorer(\"neg_mean_squared_error\")\n",
    "\n",
    "# GroupKFold considers `groups` while splitting:\n",
    "cv=GroupKFold(n_splits=2)\n",
    "\n",
    "cross_validate(ridge, X, y, groups=groups, cv=cv, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we're going to combine everything into a more realistic scenario\n",
    "- what if we tried to pass `sample_weight` to a cross validation that uses a grid search on something that can consume `sample_weight`\n",
    "- note that `Ridge().fit()` can consume `sample_weight`, but when passed into `GridSearchCV` `sample_weight` is ignored, because it is re-fitted internally\n",
    "    - so we try to pass it into the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'sample_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m scoring \u001b[38;5;241m=\u001b[39m get_scorer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m search \u001b[38;5;241m=\u001b[39m GridSearchCV(ridge, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/metadata_talk/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:191\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/inspect.py:3259\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/inspect.py:3248\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3246\u001b[0m         arguments[kwargs_param\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   3247\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3249\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   3250\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[0;31mTypeError\u001b[0m: got an unexpected keyword argument 'sample_weight'"
     ]
    }
   ],
   "source": [
    "#%%script echo skipping\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "# Ridge().fit() can consume `sample_weight`, but when passed into `GridSearchCV` `sample_weight` is ignored:\n",
    "ridge = Ridge()\n",
    "\n",
    "param_grid={\"alpha\": [0.1, 1, 10]}\n",
    "# GroupKFold considers `groups` while splitting:\n",
    "cv=GroupKFold(n_splits=2)\n",
    "# `mean_squared_error` can consume sample_weight:\n",
    "scoring = get_scorer(\"neg_mean_squared_error\")\n",
    "\n",
    "search = GridSearchCV(ridge, param_grid=param_grid, cv=cv, scoring=scoring)\n",
    "\n",
    "cross_validate(\n",
    "    search,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    sample_weight=sample_weight,\n",
    "    groups=groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we get a TypeError as `sample_weight` is not recognised\n",
    "\n",
    "- we have just witnessed the limitations on using metadata in nested structures\n",
    "\n",
    "- if we want to predict treatment efficiency for future patients in fairer, less biased models, even if past data was not gathered from a randomized trial, we want to use metadata everywhere / in the whole data science pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the metadata routing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "import sklearn\n",
    "\n",
    "with sklearn.config_context(enable_metadata_routing=True):\n",
    "    ridge = Ridge().set_fit_request(sample_weight=True)\n",
    "    \n",
    "    param_grid={\"alpha\": [0.1, 1, 10]}\n",
    "    cv=GroupKFold(n_splits=2)\n",
    "    scoring = get_scorer(\"neg_mean_squared_error\").set_score_request(sample_weight=True)\n",
    "\n",
    "    search = GridSearchCV(ridge, param_grid=param_grid, cv=cv, scoring=scoring)\n",
    "\n",
    "    cross_validate(\n",
    "        search,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        params={\"sample_weight\": sample_weight, \"groups\": groups},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- at current time, we have to enable metadata routing, because it's still experimental\n",
    "-\n",
    "- the main idea of the routing API is that we will have a centralised place where to pass the metadata, which in this case is `cross_validate`\n",
    "- our top layer tool to interact with is `cross_validate`\n",
    "- here, we pass both our metadata in: `groups` and `sample_weight`\n",
    "- \n",
    "- then, we want to define, where this metadata is routed to, that is, where it should be consumed\n",
    "- we use the `set_{method}_request()` methods to define, that this method expects which metadata to be passed\n",
    "- we do that with `set_fit_request()` on `Ridge.fit()` and with `set_score_request()` on the scorer object\n",
    "-\n",
    "- as before, we have `GroupKFold` as a cv-splitter, which accepts `groups` by design\n",
    "- and  since this is a grouped splitter, we have no choice than to pass the `groups` (and thus we don't need to set any request)\n",
    "-\n",
    "- we have enabled metadata routing, we have set the requests to where sample should be routed and we have passed the values for `sample_weight` and `groups` into `cross_validate` as `params`\n",
    "- now `cross_validate` will route our metadata to the objective function of our estimator, to the splitter used in cross validation and to the scoring metric used to evaluate the validation sets\n",
    "\n",
    "- ongoing work:\n",
    "    - with the 1.5 release of scikit-learn in May 2024, metadata routing is almost fully available\n",
    "    - in the 1.6 release (probably November), we expect all of the estimators to be compatible with metadata routing API\n",
    "    - we're working defining good default settings, so that users won't need to set all requests as currently\n",
    "    - but for your custom business metrics the flexibility you see here will also be provided\n",
    "\n",
    "- summing up: with metadata routing API:\n",
    "    - we can pass `sample_weight` and `groups` through several levels of nested-ness between the components of a data science pipeline\n",
    "    - we can combine objects from other libraries with scikit-learn estimators while still passing their metadata\n",
    "    - we can define our own custom metrics using self defined metadata ...\n",
    "    - ... and use it in a special setting with [TunedThresholdClassifier](https://scikit-learn.org/dev/auto_examples/model_selection/plot_cost_sensitive_learning.html#cost-sensitive-learning-when-gains-and-costs-are-not-constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Further information\n",
    "\n",
    "- [Notebook for this talk](https://github.com/StefanieSenger/Talks/tree/main/2024_Metadata-Routing-API)\n",
    "\n",
    "- more information on metadata routing:\n",
    "    - [User Guide on Metadata Routing](https://scikit-learn.org/stable/metadata_routing.html#metadata-routing)\n",
    "    - [Developer Guide on Metadata Routing](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_metadata_routing.html#metadata-routing)\n",
    "    - [Adrin Jalali's talk on the internal logic of metadata routing at EuroPython Conference 2023](https://www.youtube.com/watch?v=1rf6HI-pYq8)\n",
    "\n",
    "- usage of `sample_weight`:\n",
    "    - [:probabl. Whiteboard Series by Vincent Warmerdam: Improving models via subsets](https://www.youtube.com/watch?v=REIg5NH2SNc)\n",
    "    - [Blogpost by Florian Wilhelm on Inverse Probability of Treatment Weighting](https://florianwilhelm.info/2017/04/causal_inference_propensity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
